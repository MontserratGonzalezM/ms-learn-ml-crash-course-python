{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    },
    "colab": {
      "name": "11. Recurrent Neural Networks - Python.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MontserratGonzalezM/ms-learn-ml-crash-course-python/blob/master/m11.%20Recurrent%20Neural%20Networks%20-%20Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "6qi-FZndXGCd"
      },
      "source": [
        "Exercise 11 - Recurrent Neural Networks\n",
        "========\n",
        "\n",
        "A recurrent neural network (RNN) is a class of neural network that excels when your data can be treated as a sequence - such as text, music, speech recognition, connected handwriting, or data over a time period. \n",
        "\n",
        "RNN's can analyse or predict a word based on the previous words in a sentence - they allow a connection between previous information and current information.\n",
        "\n",
        "This exercise looks at implementing a LSTM RNN to generate new characters after learning from a large sample of text. LSTMs are a special type of RNN which dramatically improves the model’s ability to connect previous data to current data where there is a long gap.\n",
        "\n",
        "We will train an RNN model using a novel written by H. G. Wells - The Time Machine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVkGtRdOXGCe"
      },
      "source": [
        "Step 1\n",
        "------\n",
        "\n",
        "Let's start by loading our libraries and text file. This might take a few minutes.\n",
        "\n",
        "#### Run the cell below to import the necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "ohEbA_ABXGCf"
      },
      "source": [
        "%%capture\n",
        "# Run this!\n",
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM\n",
        "from keras.callbacks import LambdaCallback, ModelCheckpoint\n",
        "import numpy as np\n",
        "import random, sys, io, string"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzDBiRmvXGCj"
      },
      "source": [
        "#### Replace the `<addFileName>` with `The Time Machine`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "UxCEm3rgXGCk",
        "outputId": "e62632a7-ab9d-4179-d93e-7dc557f85621",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "###\n",
        "# REPLACE THE <addFileName> BELOW WITH The Time Machine\n",
        "###\n",
        "text = io.open('The Time Machine.txt', encoding = 'UTF-8').read()\n",
        "###\n",
        "\n",
        "# Let's have a look at some of the text\n",
        "print(text[0:198])\n",
        "\n",
        "# This cuts out punctuation and make all the characters lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "﻿The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
            "text length: 174201 characters\n",
            "unique characters: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdTV-NbwXGCo"
      },
      "source": [
        "Expected output:  \n",
        "```The Time Traveller (for so it will be convenient to speak of him) was expounding a recondite matter to us. His pale grey eyes shone and twinkled, and his usually pale face was flushed and animated.\n",
        "text length: 174201 characters\n",
        "unique characters: 39```\n",
        "\n",
        "Step 2\n",
        "-----\n",
        "\n",
        "Next we'll divide the text into sequences of 40 characters.\n",
        "\n",
        "Then for each sequence we'll make a training set - the following character will be the correct output for the test set.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `40`\n",
        "#### 2. `<step>` with `4`\n",
        "#### and then __run the code__. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "DM5ma7y8XGCp",
        "outputId": "718f1073-5ef3-428a-f485-ec80c7c5b2cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "###\n",
        "# REPLACE <sequenceLength> WITH 40 AND <step> WITH 4\n",
        "###\n",
        "sequence_length = 40\n",
        "step = 4\n",
        "###\n",
        "\n",
        "sequences = []\n",
        "target_chars = []\n",
        "for i in range(0, len(text) - sequence_length, step):\n",
        "    sequences.append([text[i: i + sequence_length]])\n",
        "    target_chars.append(text[i + sequence_length])\n",
        "print('number of training sequences:', len(sequences))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of training sequences: 43541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfXKdG1IXGCs"
      },
      "source": [
        "Expected output:\n",
        "`number of training sequences: 43541`\n",
        "\n",
        "#### Replace `<addSequences>` with `sequences` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "hogoJah1XGCs"
      },
      "source": [
        "# One-hot vectorise\n",
        "\n",
        "X = np.zeros((len(sequences), sequence_length, len(charset)), dtype=np.bool)\n",
        "y = np.zeros((len(sequences), len(charset)), dtype=np.bool)\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSequences> BELOW WITH sequences\n",
        "###\n",
        "for n, sequence in enumerate(sequences):\n",
        "###\n",
        "    for m, character in enumerate(list(sequence[0])):\n",
        "        X[n, m, index_from_char[character]] = 1\n",
        "    y[n, index_from_char[target_chars[n]]] = 1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgvN8eFlXGCv"
      },
      "source": [
        "Step 3\n",
        "------\n",
        "\n",
        "Let's build our model, using a single LSTM layer of 128 units. We'll keep the model simple for now, so that training does not take too long.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addLSTM>` with `LSTM`\n",
        "#### 2. `<addLayerSize>` with `128`\n",
        "#### 3. `<addSoftmaxFunction>` with `'softmax`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "075wyqYQXGCw"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "###\n",
        "# REPLACE THE <addLSTM> BELOW WITH LSTM (use uppercase) AND <addLayerSize> WITH 128\n",
        "###\n",
        "model.add(LSTM(128, input_shape = (X.shape[1], X.shape[2])))\n",
        "###\n",
        "\n",
        "###\n",
        "# REPLACE THE <addSoftmaxFunction> with 'softmax' (INCLUDING THE QUOTES)\n",
        "###\n",
        "model.add(Dense(y.shape[1], activation = 'softmax'))\n",
        "###\n",
        "\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9egn95ivXGCy"
      },
      "source": [
        "The code below generates text at the end of an epoch (one training cycle). This allows us to see how the model is performing as it trains. If you're making a large neural network with a long training time it's useful to check in on the model as see if the text generating is legible as it trains, as overtraining may occur and the output of the model turn to nonsense.\n",
        "\n",
        "The code below will also save a model if it is the best performing model, so we can use it later.\n",
        "\n",
        "#### Run the code below, but don't change it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "Musk0rXgXGCz"
      },
      "source": [
        "# Run this, but do not edit.\n",
        "# It helps generate the text and save the model epochs.\n",
        "\n",
        "# Generate new text\n",
        "def on_epoch_end(epoch, _):\n",
        "    diversity = 0.5\n",
        "    print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "    start = random.randint(0, len(text) - sequence_length - 1)\n",
        "    seed = text[start: start + sequence_length]\n",
        "    print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "    output = seed[:40].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "    print(output, end = '')\n",
        "\n",
        "    for i in range(500):\n",
        "        x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "        for t, char in enumerate(output):\n",
        "            x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "        predictions = model.predict(x_pred, verbose=0)[0]\n",
        "        exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "        next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "        next_char = char_from_index[next_index]\n",
        "\n",
        "        output = output[1:] + next_char\n",
        "\n",
        "        print(next_char, end = '')\n",
        "    print()\n",
        "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
        "\n",
        "# Save the model\n",
        "checkpoint = ModelCheckpoint('Models/model-epoch-{epoch:02d}.hdf5', \n",
        "                             monitor = 'loss', verbose = 1, save_best_only = True, mode = 'min')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPHcLm9tXGC4"
      },
      "source": [
        "The code below will start the model to train. This may take a long time. Feel free to stop the training with the `square stop button` to the right of the `Run button` in the toolbar.\n",
        "\n",
        "Later in the exercise, we will load a pretrained model.\n",
        "\n",
        "### In the cell below replace:\n",
        "#### 1. `<addPrintCallback>` with `print_callback`\n",
        "#### 2. `<addCheckpoint>` with `checkpoint`\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "aUTURSGTXGC5",
        "outputId": "cf498ba3-5620-4b7d-d41a-bcf631583630",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addPrintCallback> WITH print_callback AND <addCheckpoint> WITH checkpoint\n",
        "###\n",
        "model.fit(X, y, batch_size = 128, epochs = 3, callbacks = [print_callback, checkpoint])\n",
        "###"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.7391\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \" frugivorous also indeed i found afterwa\"\n",
            " frugivorous also indeed i found afterwat ind ew iat ae  al  ois tha an its the ti ton the t ad  ohe pe s the ap e the te the bes s an t are the sin te e e  the ad whtit e the ie le the thin te to in le s the l a li phe  ace ans the ind oi ta the the  oan  was le the as ren ree se she whe an sd yl othe cin the t aal nlol  ane to e toe in tha saes the  en  ha thed rate in le then  oas the the t the lald the  gi the  the the e ere the bor s oo  il te thes the  ase ye the a the  as t d atit id  arl he th ten wan an the t and orn the an t\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.73890, saving model to Models/model-epoch-01.hdf5\n",
            "341/341 [==============================] - 50s 147ms/step - loss: 2.7389\n",
            "Epoch 2/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.3414\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \" was seeking shelter from the heat and g\"\n",
            " was seeking shelter from the heat and gat thluang ofs foat hast oreond the sout li ad the tor ak thele le suthe the the the thart and and thes the the me and thas the that the seoret and the sat endere the in the the the tore time has lad the the and thin the the the the ther th me hot and f ee that sothe thast of thon that ind at of the wo teraplund and the seopet soin there be the bit wand tale tore sithe the mas the satinither the the the the the math of che pound the sowend the the thely sathe the the the the tale s ine the the i\n",
            "\n",
            "Epoch 00002: loss improved from 2.73890 to 2.34146, saving model to Models/model-epoch-02.hdf5\n",
            "341/341 [==============================] - 51s 151ms/step - loss: 2.3415\n",
            "Epoch 3/3\n",
            "340/341 [============================>.] - ETA: 0s - loss: 2.2129\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"a crab as large as yonder table with its\"\n",
            "a crab as large as yonder table with its and an the the moy hi nate ius ale the sames in fark in apenin whe sed the he lint the mame and lout ras mamere hed the the slathe of a the the rerethe wad toa the  and wine dertered the the the sed coulded the seand of the sard the the the the the ha loun se the the too hat le gre teane the the hed on the had ind at the mackin d mall ind the hed ind the se bas it nat in the bale the and and the mere pare satle and buld in sof ind and i tie the se mos wat sce the the serene the the the the pat \n",
            "\n",
            "Epoch 00003: loss improved from 2.34146 to 2.21290, saving model to Models/model-epoch-03.hdf5\n",
            "341/341 [==============================] - 52s 153ms/step - loss: 2.2129\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fa7cc6fd668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0FKC2hKXGC8"
      },
      "source": [
        "The output won't appear to be very good. But then, this dataset is small, and we have trained it only for a short time using a rather small RNN. How might it look if we upscaled things?\n",
        "\n",
        "Step 5\n",
        "------\n",
        "\n",
        "We could improve our model by:\n",
        "* Having a larger training set.\n",
        "* Increasing the number of LSTM units.\n",
        "* Training it for longer\n",
        "* Experimenting with difference activation functions, optimization functions etc\n",
        "\n",
        "Training this would still take far too long on most computers to see good results - so we've trained a model already for you.\n",
        "\n",
        "This model uses a different dataset - a few of the King Arthur tales pasted together. The model used:\n",
        "* sequences of 50 characters\n",
        "* Two LSTM layers (512 units each)\n",
        "* A dropout of 0.5 after each LSTM layer\n",
        "* Only 30 epochs (we'd recomend 100-200)\n",
        "\n",
        "Let's try importing this model that has already been trained.\n",
        "\n",
        "#### Replace `<addLoadModel>` with `load_model` and run the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "kSdNg3VHXGC8",
        "outputId": "33a6ab16-d152-41fc-bbb8-26342f75d8ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "print(\"loading model... \", end = '')\n",
        "\n",
        "###\n",
        "# REPLACE <addLoadModel> BELOW WITH load_model\n",
        "###\n",
        "model = load_model('/content/arthur-model-epoch-30.hdf5')\n",
        "###\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'Adam')\n",
        "###\n",
        "\n",
        "print(\"model loaded\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading model... model loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycdq_Gd9XGC_"
      },
      "source": [
        "Step 6\n",
        "-------\n",
        "\n",
        "Now let's use this model to generate some new text!\n",
        "\n",
        "#### Replace `<addFilePath>` with `'Data/Arthur tales.txt'`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "YJEWK4bWXGDA",
        "outputId": "f23c1647-369a-4b72-9dd5-9558489d9335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "###\n",
        "# REPLACE <addFilePath> BELOW WITH 'Data/Arthur tales.txt' (INCLUDING THE QUOTATION MARKS)\n",
        "###\n",
        "text = io.open('Arthur tales.txt', encoding='UTF-8').read()\n",
        "###\n",
        "\n",
        "# Cut out punctuation and make lower case\n",
        "text = text.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "# Character index dictionary\n",
        "charset = sorted(list(set(text)))\n",
        "index_from_char = dict((c, i) for i, c in enumerate(charset))\n",
        "char_from_index = dict((i, c) for i, c in enumerate(charset))\n",
        "\n",
        "print('text length: %s characters' %len(text))\n",
        "print('unique characters: %s' %len(charset))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "text length: 3645951 characters\n",
            "unique characters: 43\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBdwJw8nXGDD"
      },
      "source": [
        "### In the cell below replace:\n",
        "#### 1. `<sequenceLength>` with `50`\n",
        "#### 2. `<writeSentence>` with a sentence of your own, at least 50 characters long.\n",
        "#### 3. `<numCharsToGenerate>` with the number of characters you want to generate (choose a large number, like 1500)\n",
        "#### and then __run the code__."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": false,
        "id": "dZE796_4XGDE",
        "outputId": "e084851a-98ce-4ed6-d286-345c5e5be44a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        }
      },
      "source": [
        "# Generate text\n",
        "\n",
        "diversity = 0.5\n",
        "print('\\n### Generating text with diversity %0.2f' %(diversity))\n",
        "\n",
        "###\n",
        "# REPLACE <sequenceLength> BELOW WITH 50\n",
        "###\n",
        "sequence_length = 50\n",
        "###\n",
        "\n",
        "# Next we'll make a starting point for our text generator\n",
        "\n",
        "###\n",
        "# REPLACE <writeSentence> WITH A SENTENCE OF AT LEAST 50 CHARACTERS\n",
        "###\n",
        "seed = \"Then the Archbishop of Canterbury by Merlin's providence let purvey then of the best knights that they might get\"\n",
        "###\n",
        "\n",
        "seed = seed.lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "###\n",
        "# OR, ALTERNATIVELY, UNCOMMENT THE FOLLOWING TWO LINES AND GRAB A RANDOM STRING FROM THE TEXT FILE\n",
        "###\n",
        "\n",
        "#start = random.randint(0, len(text) - sequence_length - 1)\n",
        "#seed = text[start: start + sequence_length]\n",
        "\n",
        "###\n",
        "\n",
        "print('### Generating with seed: \"%s\"' %seed[:40])\n",
        "\n",
        "output = seed[:sequence_length].lower().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "print(output, end = '')\n",
        "\n",
        "###\n",
        "# REPLACE THE <numCharsToGenerate> BELOW WITH THE NUMBER OF CHARACTERS WE WISH TO GENERATE, e.g. 1500\n",
        "###\n",
        "for i in range(1500):\n",
        "###\n",
        "    x_pred = np.zeros((1, sequence_length, len(charset)))\n",
        "    for t, char in enumerate(output):\n",
        "        x_pred[0, t, index_from_char[char]] = 1.\n",
        "\n",
        "    predictions = model.predict(x_pred, verbose=0)[0]\n",
        "    exp_preds = np.exp(np.log(np.asarray(predictions).astype('float64')) / diversity)\n",
        "    next_index = np.argmax(np.random.multinomial(1, exp_preds / np.sum(exp_preds), 1))\n",
        "    next_char = char_from_index[next_index]\n",
        "\n",
        "    output = output[1:] + next_char\n",
        "\n",
        "    print(next_char, end = '')\n",
        "print()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "### Generating text with diversity 0.50\n",
            "### Generating with seed: \"then the archbishop of canterbury by mer\"\n",
            "then the archbishop of canterbury by merlins province and a good knight and therefore i will they for i shall be the best with thee to the lady of the land of what knights and called the castle of the door as sir gawaine with a great desert and the death of her beard and he said and the knight of the round table there was a lady of the round table and he was ready and there was a great point to the earth and after that sir bors heard a knight forward and there he put sir dinadan and said sir bors and therefore i will come to the good knight and the knight of the round table had met with him to the same road and the king was slain as the cause of the sangreal shall find me and then he said to him “it is not a worship that is the best so they said sir knight i wi"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2460f115d559>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mx_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_from_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mexp_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float64'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mnext_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_preds\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m    129\u001b[0m           method.__name__))\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1597\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1599\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1600\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWs7UOYHXGDG"
      },
      "source": [
        "How does it look? Does it seem intelligible?\n",
        "\n",
        "Conclusion\n",
        "--------\n",
        "\n",
        "We have trained an RNN that learns to predict characters based on a text sequence. We have trained a lightweight model from scratch, as well as imported a pre-trained model and generated new text from that."
      ]
    }
  ]
}